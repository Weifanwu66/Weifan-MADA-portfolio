---
title: "machine learning"
author: "Weifan Wu"
editor: visual
output: html_document
date: "02/25/2023"
---

### Using machine learning approach to predict `BodyTemp`
##Set up Libraries

```{r}
library(tidyverse)
library(here)
library(skimr)
library(tidymodels)
library(rpart)
library(glmnet)
library(ranger)
library(vip)
library(rpart.plot)
library(knitr)
```


## Load data

```{r}
data_location=here::here("fluanalysis","data","processed_data.rds")
exp_data=readRDS(file=data_location)
```


## Data Setup

```{r}
set.seed(123)
# data splitting
bodytemp_split=initial_split(exp_data,prop=0.7,strata = BodyTemp)
bodytemp_train=training(bodytemp_split)
bodytemp_test=testing(bodytemp_split)
bodytemp_train
# data re-sampling using cross-validation
bodytemp_folds=vfold_cv(bodytemp_train,v=5,repeats = 5,strata = BodyTemp)
# Create a recipe
bodytemp_rec=recipe(BodyTemp~.,data=bodytemp_train)%>%
  step_dummy(all_nominal(),-Weakness,-CoughIntensity,-Myalgia)%>%
  step_mutate(Weakness = factor(Weakness, levels = c("None","Mild","Moderate","Severe"), ordered = TRUE),
              CoughIntensity= factor(CoughIntensity, levels = c("None","Mild","Moderate","Severe"), ordered = TRUE),
              Myalgia=factor(Myalgia, levels = c("None","Mild","Moderate","Severe"), ordered = TRUE)) %>%
  step_ordinalscore(Weakness, CoughIntensity, Myalgia)%>%
  step_nzv(all_predictors(),unique_cut = 50)

```

## Creating workflow without models

```{r}
bodytemp_wf=workflow()%>%
  add_recipe(bodytemp_rec)
```


## Null model performance

```{r}
null_regression <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

null_rs <- fit_resamples(
 bodytemp_wf %>% add_model(null_regression),
  bodytemp_folds,
  metrics = metric_set(rmse),
 control=control_resamples(save_pred = TRUE)
)

null_rs%>%
  collect_metrics()
```


## Model specification for three models

```{r}
# Decision Tree
tree_spec=decision_tree(
  cost_complexity = tune(),
  tree_depth=tune()
  )%>%
  set_engine("rpart")%>%
  set_mode("regression")
# LASSO
glm_spec=linear_reg(penalty=tune(),mixture=1)%>%
  set_engine("glmnet")

# Random forest
rf_spec=
  rand_forest(mtry=tune(),min_n=tune(),trees=1000)%>%
  set_engine("ranger",num.threads=cores)%>%
  set_mode("regression")
```


## Creating workflow for each model

```{r}
# Decision Tree
tree_wf=bodytemp_wf%>%
  add_model(tree_spec)
# LASSO
glm_wf=bodytemp_wf%>%
  add_model(glm_spec)
# Random forest
rf_wf=bodytemp_wf%>%
  add_model(rf_spec)
```



## Creating regular grid of values for each hyperparameters in three models

```{r}
# Decision Tree
tree_grid=grid_regular(cost_complexity(),
                       tree_depth(),
                       levels=5)
tree_grid
# LASSO
glm_grid=tibble(penalty=10^seq(-4,-1,length.out=30))
glm_grid%>%
  top_n(-5)
glm_grid%>%
  top_n(5)
# Random forest
# using a space-filling design to tune later

```


## Train and tune the models

```{r}
# Decision Tree
tree_rs=tree_wf%>%
  tune_grid(
    resamples=bodytemp_folds,
    grid=tree_grid
  )
tree_rs

# LASSO
glm_rs=glm_wf%>%
  tune_grid(
    resamples=bodytemp_folds,
    grid=glm_grid,
    control=control_grid(save_pred = TRUE),
    metrics=metric_set(rmse)
  )
glm_rs

# Random forest
rf_rs=rf_wf%>%
  tune_grid(
    resamples=bodytemp_folds,
    grid=25,
    control=control_grid(save_pred=TRUE),
    metrics = metric_set(rmse)
  )
rf_rs
```

## Evaluate the performance and finalize the model
### Decision trees

```{r}
# plotting the result
tree_rs%>%
autoplot()
# Selecting the best tuning parameter based on rmse
best_tree=tree_rs%>%
  select_best("rmse")
# Finalize the workflow
tree_final=
  tree_wf%>%
  finalize_workflow(best_tree)
tree_final
# Fitting training data
tree_fit=tree_final%>%
  fit(bodyemp_train)
# Predicted value
tree_predict=tree_final%>%
  fit(bodyemp_train)%>%
  predict(bodytemp_train)
tree_predict
# Plotting the true values vs predicted values
bodytemp_train%>%
  select(BodyTemp)%>%
  bind_cols(tree_predict)%>%
  ggplot(aes(BodyTemp,.pred))+
  geom_jitter()

# Plotting the residuals
# Calculate residuals
tree_resid <- bodytemp_train$BodyTemp - tree_predict
# Create a data frame with the residuals and predicted values
resid_df_tree <- data.frame(tree_resid, tree_predict)
colnames(resid_df_tree) <- c("residuals", "predicted_values")
# Draw the residual plot
ggplot(resid_df_tree, aes(x = predicted_values,y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x="Predicted values",y="Residuals",title="Residual Plot for Decision Tree Model") 
```


### LASSO

```{r}
# plotting the result
glm_rs%>%
autoplot()
# show the best tuning parameter
glm_rs%>%
 show_best("rmse")
# Selecting the best tuning parameter based on rmse
best_glm=glm_rs%>%
  select_best("rmse")
best_glm
# Finalize the workflow
glm_final=
  glm_wf%>%
  finalize_workflow(best_glm)
# Fitting training data
glm_fit=glm_final%>%
  fit(bodyemp_train)
# Predicted value
glm_predict=glm_final%>%
  fit(bodyemp_train)%>%
  predict(bodytemp_train)
glm_predict
# Plotting the true values vs predicted values
bodytemp_train%>%
  select(BodyTemp)%>%
  bind_cols(glm_predict)%>%
  ggplot(aes(BodyTemp,.pred))+
  geom_jitter()
# Residuals analysis
# Calculate residuals
glm_resid <- bodytemp_train$BodyTemp - glm_predict
# Create a data frame with the residuals and predicted values
resid_df_glm <- data.frame(glm_resid, glm_predict)
colnames(resid_df_glm) <- c("residuals", "predicted_values")
# Draw the residual plot
ggplot(resid_df_glm, aes(x = predicted_values,y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x="Predicted values",y="Residuals",title="Residual Plot for GLM Model") 

```

### Random forest

```{r}
# plotting the result
rf_rs%>%
autoplot()
# show the best tuning parameter
rf_rs%>%
 show_best("rmse")
# Selecting the best tuning parameter based on rmse
best_rf=rf_rs%>%
  select_best("rmse")
best_rf
# Finalize the workflow
rf_final=
  rf_wf%>%
  finalize_workflow(best_rf)
rf_final
# Fitting training data
rf_fit=rf_final%>%
  fit(bodyemp_train)
# Predicted value
rf_predict=glm_final%>%
  fit(bodyemp_train)%>%
  predict(bodytemp_train)
rf_predict
# Plotting the true values vs predicted values
bodytemp_train%>%
  select(BodyTemp)%>%
  bind_cols(rf_predict)%>%
  ggplot(aes(BodyTemp,.pred))+
  geom_jitter()

# Plotting the residuals
# Calculate residuals
rf_resid <- bodytemp_train$BodyTemp - rf_predict
# Create a data frame with the residuals and predicted values
resid_df_rf <- data.frame(rf_resid, rf_predict)
colnames(resid_df_rf) <- c("residuals", "predicted_values")
# Draw the residual plot
ggplot(resid_df_rf, aes(x = predicted_values,y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x="Predicted values",y="Residuals",title="Residual Plot for Random Forest Model")
```


## Compare three models with the null model

```{r}
tree_metrics=tree_rs%>%
 show_best("rmse")%>%
  filter(.config=="Preprocessor1_Model01")
glm_metrics=glm_rs%>%
 show_best("rmse")%>%
  filter(.config=="Preprocessor1_Model27")
rf_metrics=rf_rs%>%
 show_best("rmse")%>%
  filter(.config=="Preprocessor1_Model22")
null_rs%>%
  collect_metrics()%>%
  mutate(model="Null")%>%
  bind_rows(tree_metrics%>%
              mutate(model="tree"),
            glm_metrics%>%
              mutate(model="LASSO"),
            rf_metrics%>%
              mutate(model="random forest")
    
  )%>%
  select(c(1:6),model)%>%
  kable()
```

## Final model selection
### After comparing four models, LASSO model is the best model since it has the lowest rmse. Although the standard error is a bit higher than that of random forest.

```{r}
glm_fit_final=glm_final%>%
  last_fit(bodytemp_split)
glm_fit_final%>%
  collect_metrics()
glm_predict_final=glm_fit_final%>%
  collect_predictions()
glm_fit_final%>%
  extract_fit_engine()%>%
  vip()
# Plotting the true values vs predicted values
glm_predict_final%>%
  ggplot(aes(BodyTemp,.pred))+
  geom_jitter()+
  geom_smooth()
## Plot residuals
final_resid=glm_predict_final$BodyTemp-glm_predict_final$.pred
final_resid_df=data.frame(final_resid,glm_predict_final$.pred)
colnames(final_resid_df)=c("residuals", "predicted_values")
ggplot(final_resid_df,aes(predicted_values,residuals))+
  geom_point()+
  geom_hline(yintercept=0,linetype="dashed")+
   labs(x="Predicted values",y="Residuals",title="Residual Plot for Final Model LASSO")
```

## In conclusion, the best model LASSO has  rmse of 1.155 when it fits testing data. The residuals seems to be randomly distributed on the residual plot


